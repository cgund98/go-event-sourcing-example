# go-event-sourcing-example

This repository demonstrates how to build an **eventâ€‘sourced system** in Go.

The example application is a simplified **order management system**, inspired by platforms like eBay. It supports:

1. Tracking payment status  
2. Monitoring shipping updates  
3. Canceling orders when needed  

---

### Why Event Sourcing?

Event sourcing is an architectural pattern that stores every change to an applicationâ€™s state as a sequence of events.  
Itâ€™s particularly wellâ€‘suited for systems that require:

- **Reliability** â€“ every state change is recorded and recoverable  
- **Transparency** â€“ a complete audit trail is available  
- **Scalability** â€“ event logs can be processed in parallel and replayed  

For an order system, this approach provides a **robust foundation** for audits, reconciliation, and future feature growth.

---

## ğŸ— Architecture Overview

This project combines **event sourcing** with **CQRS (Command Query Responsibility Segregation)**.

### Components
1. **Postgres** â€“ Serves as both the **event store** and the **projection database**.  
2. **Kafka** â€“ Acts as the **message bus** for notifying consumers when new events are committed.

### Why Postgres as the event store instead of Kafka?  
Kafka excels at streaming, but itâ€™s not ideal for **perâ€‘aggregate queries**. In this system, we frequently need to load **all events for a given `OrderID`** to rehydrate state or validate commands.  

Doing this directly from Kafka would require either:  
- Scanning the entire topic (slow and inefficient), or  
- Creating a **partition per Order** (operationally infeasible at scale).  

By writing events to **Postgres** first, we get:  
- Fast, indexed queries for any aggregate.  
- Transactional guarantees that events and downstream notifications are consistent.

### Highâ€‘level flow
1. **Commands** (e.g. `PlaceOrder`, `CancelOrder`) write new events to a Postgres **event table**.  
2. In the **same transaction**, a **notification** is published to a Kafka topic.  
3. **Consumers** read from Kafka and update **projection models** (readâ€‘optimized views).  
4. **Projections** are stored back in Postgres and exposed to clients via **gRPC** and **JSON HTTP APIs**.

### Event Syntax

We're using **Protocol Buffers (protobuf)** to define and manage our event schemas, as they provide several unique benefits:

1. **Languageâ€‘agnostic** â€“ We can compile stubs for multiple client languages. For example, if we later introduce a Python consumer, it can share the same schema definitions.
2. **Strong type support** â€“ Ensures our events are wellâ€‘structured and typeâ€‘safe.
3. **Schema evolution support** â€“ Combined with a schema registry, protobuf allows us to evolve event types in a **backwardsâ€‘compatible** way.

That said, protobuf comes with a few tradeâ€‘offs:

1. **Limited thirdâ€‘party tooling** â€“ Many analytics or BI tools donâ€™t natively understand protobuf, meaning weâ€™d need to serialize to JSON (or another format) for querying in a data warehouse.
2. **Not humanâ€‘readable** â€“ The raw protobuf binary format isnâ€™t easily interpreted in systems like Postgres; events must be parsed to make sense of their contents.
